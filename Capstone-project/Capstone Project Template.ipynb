{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project consists of creating a data model following a star schema that contains data about immigrants moving to/from US cities. The data model will contain not only information about immigrants but also information about US cities demographics as well as their airports.<br>\n",
    "The main challenge of this project is to clean and transform data from different sources to generate a data model that represents a unique ground truth about the data and that will be used for analytical processes.<br>\n",
    "Once the data model is ready for processing, we can get insights such as what cities are prefered by immigrants and we can even filter by age, race and many other dimensions. We can also check if the available airports in the cities can handle the flow of immigrants.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "\n",
    "During the next steps, I will read the data and check if there are any anomalies such as missing data and duplicates.<br> Next, I will define a data model following a star schema, then, I will run data pipelines to aggregate the data by cities and generate dimension and fact tables.<br> Finally, I will check the quality of the data and write the tables as parquet files on S3 which is going to be the final form or output of this project. During these steps I will use pandas and pyspark for data processing. pyspark will be extremely helpful especially when processing the immigration dataset which contains over 3 million rows/samples.\n",
    "\n",
    "#### Data Description\n",
    "\n",
    "Our data is derived from 3 sources or files as follows:\n",
    "* [I94 Immigration Data](https://travel.trade.gov/research/reports/i94/historical/2016.html): This data comes from the US National Tourism and Trade Office. It contains detailed information about visitors arrival/departure to/from US states during the period of april 2016.\n",
    "* [U.S. City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/): This data comes from OpenSoft. It contains detailed demographic statistics for each state and city in the US.\n",
    "* [Airport Code Table](https://datahub.io/core/airport-codes#data): This data contains table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = sparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read immigrants data\n",
    "\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "#fname = 'csv_data/immigration_data_sample.csv'\n",
    "immigration = spark.read.format('com.github.saurfang.sas.spark').load(fname, inferSchema=True)\n",
    "#immigration = spark.read.csv(fname, header=True, inferSchema=True).drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read immigration data\n",
    "\n",
    "fname = 'csv_data/us-cities-demographics.csv'\n",
    "demographics = spark.read.csv(fname, sep=';', header='true', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read airports data\n",
    "\n",
    "fname = 'csv_data/airport-codes_csv.csv'\n",
    "airport = spark.read.csv(fname, header='true', inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Cleaning Steps\n",
    "Steps necessary to clean the data\n",
    "\n",
    "1. check and drop duplicated data\n",
    "2. drop features with nans over 80%\n",
    "3. drop rows with only nans\n",
    "4. check statistical anomalies and fields format(iso_region for airport data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Explore and Clean Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\t\t*********** AIRPORT Basic Data Exploration and Cleaning ***********\n",
      "\n",
      "----------- AIRPORT Top 5 Rows --------------\n",
      "\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+----------+--------------------+\n",
      "| 06IN|       closed|Ellis Fly-In Airport|         575|       NA|         US|     US-IN|   Blackhawk|    null|      null|-87.303596, 39.28...|\n",
      "| 06VA|small_airport|   Mount Horeb Field|        1160|       NA|         US|     US-VA|    Grottoes|    06VA|      06VA|-78.8553009033203...|\n",
      "| 0LA0|     heliport|West Hackberry He...|          10|       NA|         US|     US-LA|   Hackberry|    0LA0|      0LA0|-93.4001998901367...|\n",
      "| 0MD6|small_airport|     Walters Airport|         750|       NA|         US|     US-MD|  Mount Airy|    0MD6|      0MD6|-77.1057968139648...|\n",
      "| 0OH7|small_airport|       Apple Airport|        1000|       NA|         US|     US-OH|       Piqua|    0OH7|      0OH7|-84.1718978881836...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "----------- AIRPORT Data Schema --------------\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "---------- Data Shape For AIRPORT ----------\n",
      "        AIRPORT Data Shape : Rows=55075 Columns=11\n",
      "\n",
      "---------- Data Duplicates For AIRPORT ----------\n",
      "        AIRPORT Duplicated values = 0\n",
      "\n",
      "---------- Missing Data Percentage For AIRPORT ----------\n",
      "+-----+----+----+-----------------+---------+-----------+----------+-------------------+-------------------+------------------+------------------+-----------+\n",
      "|ident|type|name|     elevation_ft|continent|iso_country|iso_region|       municipality|           gps_code|         iata_code|        local_code|coordinates|\n",
      "+-----+----+----+-----------------+---------+-----------+----------+-------------------+-------------------+------------------+------------------+-----------+\n",
      "|  0.0| 0.0| 0.0|0.127208352246936|      0.0|        0.0|       0.0|0.10305946436677259|0.25501588742623693|0.8331547889241943|0.4791466182478438|        0.0|\n",
      "+-----+----+----+-----------------+---------+-----------+----------+-------------------+-------------------+------------------+------------------+-----------+\n",
      "\n",
      " Dropped features : ['iata_code']\n"
     ]
    }
   ],
   "source": [
    "airport = dataExplorationCleanining(\n",
    "    airport,\n",
    "    'airport',\n",
    "    duplicates_fields=['ident'],\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### checking fields anomalies for airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      elevation_ft|\n",
      "+-------+------------------+\n",
      "|  count|             48069|\n",
      "|   mean|1240.7896773388254|\n",
      "| stddev| 1602.363459348408|\n",
      "|    min|             -1266|\n",
      "|    25%|               205|\n",
      "|    50%|               718|\n",
      "|    75%|              1498|\n",
      "|    max|             22000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport.select(['elevation_ft']).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9fbbd0d080>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(airport.select(['elevation_ft']).toPandas().dropna().values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(elevation_ft=22000, type='heliport', name='Siachen Glacier AFS Airport'),\n",
       " Row(elevation_ft=16200, type='small_airport', name='Daulat Beg Oldi Advanced Landing Ground')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.select(['elevation_ft', 'type', 'name']).where(col('elevation_ft')  > 15000).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explore us cities demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\t\t*********** DEMOGRAPHICS Basic Data Exploration and Cleaning ***********\n",
      "\n",
      "----------- DEMOGRAPHICS Top 5 Rows --------------\n",
      "\n",
      "+----------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|     State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|         Bayonne|New Jersey|      39.7|          32705|            33598|           66303|              2225|       21899|                  2.62|        NJ|Black or African-...| 7581|\n",
      "|          Canton|      Ohio|      33.4|          34476|            37419|           71895|              3404|        2204|                  2.32|        OH|               White|53571|\n",
      "|        Carlsbad|California|      42.1|          55119|            58347|          113466|              6031|       17689|                  2.68|        CA|Black or African-...|  876|\n",
      "|Colorado Springs|  Colorado|      34.8|         225544|           231018|          456562|             49291|       35320|                  2.48|        CO|               Asian|22619|\n",
      "|      Des Moines|      Iowa|      34.5|         103726|           106591|          210317|             11780|       23857|                  2.41|        IA|Black or African-...|26353|\n",
      "+----------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "----------- DEMOGRAPHICS Data Schema --------------\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "---------- Data Shape For DEMOGRAPHICS ----------\n",
      "        DEMOGRAPHICS Data Shape : Rows=2891 Columns=12\n",
      "\n",
      "---------- Data Duplicates For DEMOGRAPHICS ----------\n",
      "        DEMOGRAPHICS Duplicated values = 0\n",
      "\n",
      "---------- Missing Data Percentage For DEMOGRAPHICS ----------\n",
      "+----+-----+----------+--------------------+--------------------+----------------+--------------------+--------------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|     Male Population|   Female Population|Total Population|  Number of Veterans|        Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+--------------------+--------------------+----------------+--------------------+--------------------+----------------------+----------+----+-----+\n",
      "| 0.0|  0.0|       0.0|0.001037703216879...|0.001037703216879...|             0.0|0.004496713939813214|0.004496713939813214|  0.005534417156693186|       0.0| 0.0|  0.0|\n",
      "+----+-----+----------+--------------------+--------------------+----------------+--------------------+--------------------+----------------------+----------+----+-----+\n",
      "\n",
      " Dropped features : []\n"
     ]
    }
   ],
   "source": [
    "demographics = dataExplorationCleanining(\n",
    "    demographics,\n",
    "    'demographics',\n",
    "    duplicates_fields=['City', 'State Code', 'Race'],\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+------------------+------------------+------------------+----------------------+------------------+\n",
      "|summary|       Median Age|  Male Population| Female Population|  Total Population|Number of Veterans|      Foreign-born|Average Household Size|             Count|\n",
      "+-------+-----------------+-----------------+------------------+------------------+------------------+------------------+----------------------+------------------+\n",
      "|  count|             2891|             2888|              2888|              2891|              2878|              2878|                  2875|              2891|\n",
      "|   mean|35.49488066413007|97328.42624653739|101769.63088642659|198966.77931511588| 9367.832522585128|40653.598679638635|     2.742542608695653| 48963.77447250087|\n",
      "| stddev|4.401616730099887| 216299.936928733|231564.57257148283|447555.92963359045| 13211.21992386409| 155749.1036650984|    0.4332910878973049|144385.58856460612|\n",
      "|    min|             22.9|            29281|             27348|             63215|               416|               861|                   2.0|                98|\n",
      "|    25%|             32.8|            39289|             41227|             80429|              3739|              9224|                  2.43|              3434|\n",
      "|    50%|             35.3|            52336|             53809|            106782|              5397|             18822|                  2.65|             13780|\n",
      "|    75%|             38.0|            86596|             89604|            175232|              9368|             34003|                  2.95|             54496|\n",
      "|    max|             70.5|          4081698|           4468707|           8550405|            156961|           3212500|                  4.98|           3835726|\n",
      "+-------+-----------------+-----------------+------------------+------------------+------------------+------------------+----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_fields=[\n",
    "    'Median Age', 'Male Population',\n",
    "    'Female Population', 'Total Population',\n",
    "    'Number of Veterans', 'Foreign-born',\n",
    "    'Average Household Size', 'Count'\n",
    "]\n",
    "demographics.select(selected_fields).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explore immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\t\t*********** IMMIGRATION Basic Data Exploration and Cleaning ***********\n",
      "\n",
      "----------- IMMIGRATION Top 5 Rows --------------\n",
      "\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+---------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|entdepa|entdepd|matflag|biryear| dtaddto|gender|airline|         admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+---------------+-----+--------+\n",
      "|299.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20550.0|  54.0|    2.0|  1.0|20160401|    null|      O|      O|      M| 1962.0|06292016|  null|     OS|5.5425872433E10|00087|      WT|\n",
      "|305.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20555.0|  63.0|    2.0|  1.0|20160401|    null|      O|      O|      M| 1953.0|06292016|  null|     OS|5.5425817433E10|00087|      WT|\n",
      "|496.0|2016.0|   4.0| 103.0| 103.0|    CHI|20545.0|    1.0|     IL|20548.0|  64.0|    1.0|  1.0|20160401|    null|      O|      O|      M| 1952.0|06292016|  null|     OS|5.5428623333E10|00065|      WB|\n",
      "|558.0|2016.0|   4.0| 103.0| 103.0|    SFR|20545.0|    1.0|     CA|20547.0|  42.0|    1.0|  1.0|20160401|    null|      G|      O|      M| 1974.0|06292016|     M|     LH|5.5433311133E10|00454|      WB|\n",
      "|596.0|2016.0|   4.0| 103.0| 103.0|    NAS|20545.0|    1.0|     FL|20547.0|  24.0|    2.0|  1.0|20160401|    null|      G|      N|      M| 1992.0|06292016|     M|     UP|5.5406105433E10|00221|      WT|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "----------- IMMIGRATION Data Schema --------------\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "---------- Data Shape For IMMIGRATION ----------\n",
      "        IMMIGRATION Data Shape : Rows=3096313 Columns=25\n",
      "\n",
      "---------- Data Duplicates For IMMIGRATION ----------\n",
      "        IMMIGRATION Duplicated values = 0\n",
      "\n",
      "---------- Missing Data Percentage For IMMIGRATION ----------\n",
      "+-----+-----+------+------+------+-------+-------+--------------------+-------------------+-------------------+--------------------+-------+-----+--------------------+------------------+------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------------+------------------+-------------------+------+--------------------+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|             i94mode|            i94addr|            depdate|              i94bir|i94visa|count|            dtadfile|          visapost|             occup|             entdepa|             entdepd|           entdepu|             matflag|             biryear|             dtaddto|           gender|            insnum|            airline|admnum|               fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+--------------------+-------------------+-------------------+--------------------+-------+-----+--------------------+------------------+------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------------+------------------+-------------------+------+--------------------+--------+\n",
      "|  0.0|  0.0|   0.0|   0.0|   0.0|    0.0|    0.0|7.718857880324114E-5|0.04928183940060323|0.04600859150867499|2.590177414234284...|    0.0|  0.0|3.229647648671177...|0.6075774639062653|0.9973755883206898|7.686561403837402E-5|0.044707689435790246|0.9998733978121721|0.044707689435790246|2.590177414234284...|1.540541928416151...|0.133794290176736|0.9632763225164898|0.02700857439154246|   0.0|0.006313638188387285|     0.0|\n",
      "+-----+-----+------+------+------+-------+-------+--------------------+-------------------+-------------------+--------------------+-------+-----+--------------------+------------------+------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------------+------------------+-------------------+------+--------------------+--------+\n",
      "\n",
      " Dropped features : ['occup', 'entdepu', 'insnum']\n"
     ]
    }
   ],
   "source": [
    "immigration = dataExplorationCleanining(\n",
    "    immigration,\n",
    "    'immigration',\n",
    "    duplicates_fields=['cicid'],\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![image](https://drive.google.com/uc?export=view&id=1krP58ZqA2qFRXoXVvoxK-s8Cc18YBDO7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data model is a star schema model. It consists of 3 dimension tables:<br>\n",
    "- dim_cities_airport: contains airport information aggregated by cities.\n",
    "- dim_cities_demographics: contains statistical data about cities demographics and aggregated by cities.\n",
    "- dim_time: contains features extracted from the immigration dates such as year, month, weekday etc.<br>\n",
    "As well as a fact table (fact_immigration) which contains information about each immigrant.<br>\n",
    "\n",
    "The star schema model was the preferred choice as we can get analytical insights using a few joins.<br>\n",
    "Also, the aggregation by cities of both dim_cities_airport and dim_cities_demographics dimension tables was a good choice for this model, as it is the only possible way to link the immigration data with both dimensions (airports and cities demographics).<br>\n",
    "Aggregating airports by cities made it possible to link each row from the fact table (extracted from the immigration dataset) with each row from both dim_cities_airport and dim_cities_demographics dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The steps necessary to pipeline the data into the chosen data model are as follows:\n",
    "- Generate the cities airport dimension table (<b>dim_cities_airports</b>):\n",
    "    1. Check and rectify the iso_region column format and filter the airports to keep only U.S airports.\n",
    "    2. Drop any nan value in the \"municipality\" feature and transform it to lower case.\n",
    "    3. Select airport fields and rename them, generate primary key.\n",
    "    4. Aggregate airports by cities.\n",
    "    5. Create the dim_cities_airport view\n",
    "<br>\n",
    "\n",
    "- Generate the cities demographics dimension table (<b>dim_cities_demographics</b>):\n",
    "    1. Generating race features and aggregate them by cities.\n",
    "    2. generating primary key composed of cities and state_codes.\n",
    "    3. rename columns\n",
    "    4. create the dim_cities_demographics view\n",
    "<br>\n",
    " \n",
    "- Generate the time dimension table (<b>dim_time</b>):\n",
    "    1. Drop nans from arrival and departure dates (from immigration dataset) and convert them from SAS to date format.\n",
    "    2. Extracts features suchs as month, weekofyear, isweekend ...etc. from both arrival and departure date.\n",
    "    3. Create the dim_time view.\n",
    "<br>\n",
    "\n",
    "- Generate the immigration fact table (<b>fact_immigration</b>):\n",
    "    1. Extract a map of (field --> value) from the I94_SAS_Labels_Descriptions.SAS file and decode columns of the immigration dataset.\n",
    "    2. Extracts the city name from the 'i94port' column after decoding in step1.\n",
    "    3. Drop nans from arrival and departure dates (from immigration dataset) and convert them from SAS to date format.\n",
    "    4. Select a set of columns to be included in the fact table.\n",
    "    5. Set all city values to lower case and all state_code values to upper case.\n",
    "    6. Create the fact_immigration view and join it with the time, cities_dimographics and cities_airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Pipelines to Model the Data \n",
    "#### 4.1 Data model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_dim_cities_airports(spark, airport):\n",
    "    \"\"\"\n",
    "    Aggregates airport dataset by cities and generates the cities_airport dimension.\n",
    "    ---\n",
    "    \n",
    "    args:\n",
    "        spark(SparkSession): a SparkSession object used to run spark tasks on a distributed cluster. \n",
    "        airport(Spark.DataFrame): a spark dataframe of airports data.\n",
    "        \n",
    "    returns:\n",
    "        dim_cities_airport(Spark.DataFrame): cities_airport dimension table.\n",
    "    \"\"\"\n",
    "    \n",
    "    #airport.createOrReplaceTempView('airport')\n",
    "\n",
    "    ##### step1: check and rectify the iso_region column format and filter the airports to keep only U.S airports. \n",
    "    airport = airport.withColumn('iso_region', IsoRegionFormatCheck('iso_region'))    \n",
    "    airport = airport.filter(col('iso_country')=='US')\n",
    "    \n",
    "    ###### step2: drop any nan value in the \"municipality\" feature and transform it to lower case.\n",
    "    airport = airport.withColumn('municipality', lower(col('municipality')))\n",
    "    airport = airport.dropna(subset=['municipality'])\n",
    "    \n",
    "    \n",
    "    ###### step3: select airport fields and rename them, generate primary key.\n",
    "    airport.createOrReplaceTempView('airport')\n",
    "    airport = spark.sql(\"\"\"\n",
    "        SELECT CONCAT(SUBSTR(iso_region, 4, 2), ', ', municipality) dim_airport_pk, \n",
    "        elevation_ft, name, type, municipality city, \n",
    "        SUBSTR(iso_region, 4, 2) state_code\n",
    "        From airport\n",
    "    \"\"\")\n",
    "    \n",
    "    ##### step4: aggregate airports by cities.\n",
    "    airport_pd = airport.toPandas()\n",
    "    dim_cities_airport = pd.concat(\n",
    "        [\n",
    "            pd.get_dummies(airport_pd.type),\n",
    "            airport_pd[[\n",
    "                'dim_airport_pk','state_code', \n",
    "                'city', 'elevation_ft', 'name'\n",
    "            ]]\n",
    "        ]\n",
    "        , axis=1\n",
    "    ).groupby(\n",
    "        ['dim_airport_pk', 'state_code', 'city']\n",
    "        , as_index=False\n",
    "    ).agg({\n",
    "        'large_airport':sum,\n",
    "        'medium_airport':sum,\n",
    "        'small_airport':sum,\n",
    "        'balloonport':sum,\n",
    "        'seaplane_base':sum,\n",
    "        'heliport':sum,\n",
    "        'closed':sum,\n",
    "        'name':'count',\n",
    "        'elevation_ft':'mean',\n",
    "    }).rename(\n",
    "        {\n",
    "        'large_airport':'total_large_airport',\n",
    "        'medium_airport':'total_medium_airport',\n",
    "        'small_airport':'total_small_airport',\n",
    "        'balloonport':'total_balloonport',\n",
    "        'seaplane_base':'total_seaplane_base',\n",
    "        'heliport':'total_heliport',\n",
    "        'closed':'total_closed',\n",
    "        'name':'total_airports',\n",
    "        'elevation_ft':'avg_elevation_ft',\n",
    "        }\n",
    "        , axis=1\n",
    "    )\n",
    "    \n",
    "    #### step5: create dim_cities_airport view.\n",
    "    dim_cities_airport = spark.createDataFrame(dim_cities_airport)\n",
    "    dim_cities_airport.createOrReplaceTempView('dim_cities_airport')\n",
    "    \n",
    "    return dim_cities_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_dim_cities_demographics(spark, demographics):\n",
    "    \"\"\"\n",
    "    Aggregates data by cities and generates the cities_demographics dimension.\n",
    "    ---\n",
    "    \n",
    "    args:\n",
    "        spark(SparkSession): a SparkSession object used to run spark tasks on a distributed cluster. \n",
    "        demographics(Spark.DataFrame): a spark dataframe of demographics data.\n",
    "    \n",
    "    returns:\n",
    "        dim_cities_demographics(Spark.DataFrame): cities_demographics dimension table. \n",
    "    \"\"\"\n",
    "    \n",
    "    #### step1: generating race features and aggregate them by cities.\n",
    "    demographics_pd = demographics.toPandas()\n",
    "\n",
    "    cities_race_stats = pd.get_dummies(\n",
    "        demographics_pd['Race'],\n",
    "        prefix='', \n",
    "        prefix_sep=''\n",
    "    ).multiply(\n",
    "        demographics_pd.Count, \n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    cities_race_stats = pd.concat(\n",
    "        [\n",
    "            demographics_pd[['State Code', 'City']], \n",
    "            cities_race_stats\n",
    "        ]\n",
    "        , axis=1\n",
    "    ).groupby(\n",
    "        ['State Code', 'City']\n",
    "        , as_index=False\n",
    "    ).sum()\n",
    "\n",
    "    dim_cities_demographics = demographics_pd.drop(\n",
    "        ['Race', 'Count'], axis=1\n",
    "    ).drop_duplicates()\n",
    "    \n",
    "    dim_cities_demographics = dim_cities_demographics.merge(\n",
    "    cities_race_stats, \n",
    "    on=['City', 'State Code'], \n",
    "    how='inner' \n",
    "    )\n",
    "\n",
    "    \n",
    "    #### step2: generating primary key composed of cities and state_codes.\n",
    "    dim_cities_demographics.City = dim_cities_demographics.City.str.lower()\n",
    "    dim_cities_demographics['State Code'] = dim_cities_demographics['State Code'].str.upper()\n",
    "    dim_cities_demographics['dim_cities_demographics_pk'] = dim_cities_demographics['State Code']\\\n",
    "                                                        + ', '\\\n",
    "                                                        + dim_cities_demographics['City']\n",
    "\n",
    "    #### step3: rename columns.\n",
    "    column_labels = {\n",
    "        'State Code':'state_code', \n",
    "        'State':'state', \n",
    "        'City':'city', \n",
    "        'Median Age':'median_age', \n",
    "        'Male Population':'male_population',\n",
    "        'Female Population':'female_population', \n",
    "        'Total Population':'total_population',\n",
    "        'Number of Veterans':'number_of_veterans', \n",
    "        'Foreign-born':'foreign_born', \n",
    "        'Average Household Size':'average_household_size',\n",
    "        'American Indian and Alaska Native':'american_indian_and_alaska_native_population', \n",
    "        'Asian':'asian_population', \n",
    "        'Black or African-American':'black_or_african_american_population', \n",
    "        'Hispanic or Latino':'hispanic_or_latino_population', \n",
    "        'White':'white_population'\n",
    "    }\n",
    "    dim_cities_demographics.rename(column_labels, axis=1, inplace=True)\n",
    "    \n",
    "    #### step4 : create dim_cities_demographics view.\n",
    "    dim_cities_demographics = spark.createDataFrame(dim_cities_demographics)\n",
    "    dim_cities_demographics.createOrReplaceTempView('dim_cities_demographics')\n",
    "    \n",
    "    return dim_cities_demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_dim_time(spark, immigration):\n",
    "    \"\"\"\n",
    "    Generates the time dimension which includes all arrival and departure\n",
    "    dates from the immigration data.\n",
    "    ---\n",
    "    \n",
    "    args:\n",
    "        spark(SparkSession): a SparkSession object used to run spark tasks on a distributed cluster. \n",
    "        immigration(Spark.DataFrame): a spark dataframe of immigration data.\n",
    "    \n",
    "    returns:\n",
    "        dim_time(Spark.DataFrame): time dimension table.\n",
    "    \"\"\"\n",
    "    \n",
    "    #### step1: drop nans from arrival and departure dates (from immigration dataset) and convert them from SAS to date format.\n",
    "    date_fields = ['arrdate', 'depdate']\n",
    "    immigration = immigration.dropna(subset=(date_fields))\n",
    "    immigration = immigration.withColumn('arrdate', convertSaS_toDate(col('arrdate')))\n",
    "    immigration = immigration.withColumn('depdate', convertSaS_toDate(col('depdate')))\n",
    "    immigration.createOrReplaceTempView('immigration')\n",
    " \n",
    "    #### step2: extracts features suchs as month, weekofyear, isweekend ...etc. from both arrival and departure date.\n",
    "    dim_time = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            arrival_date_pk, \n",
    "            YEAR(arrival_date_pk) as year, \n",
    "            MONTH(arrival_date_pk) as month, \n",
    "            DAY(arrival_date_pk) as day, \n",
    "            WEEKOFYEAR(arrival_date_pk) as weekofyear, \n",
    "            DAYOFWEEK(arrival_date_pk) as weekday, \n",
    "            QUARTER(arrival_date_pk) as season,\n",
    "            (CASE\n",
    "               WHEN DAYOFWEEK(arrival_date_pk) IN (7,1)\n",
    "               THEN 1\n",
    "               ELSE 0\n",
    "            END) as is_weekend\n",
    "        FROM (\n",
    "            SELECT DISTINCT arrdate as arrival_date_pk\n",
    "            FROM immigration\n",
    "            WHERE arrdate IS NOT NULL\n",
    "            UNION\n",
    "            SELECT DISTINCT depdate as arrival_date_pk\n",
    "            FROM immigration\n",
    "            WHERE depdate IS NOT NULL\n",
    "        )\n",
    "        ORDER BY arrival_date_pk ASC\n",
    "    \"\"\")\n",
    "    \n",
    "    #### step3: create a view of the time dimension. \n",
    "    dim_time.createOrReplaceTempView('dim_time')\n",
    "    return dim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def  process_fact_immigration(spark, immigration):\n",
    "    \"\"\"\n",
    "    Process immigration data to generate the immigration fact table\n",
    "    and join it with the dimension tables to get the foreign keys.\n",
    "    ---\n",
    "    \n",
    "    args:\n",
    "        spark(SparkSession): a SparkSession object used to run spark tasks on a distributed cluster. \n",
    "        immigration(Spark.DataFrame): a spark dataframe of immigration data.\n",
    "        \n",
    "    returns:\n",
    "        fact_immigration(Spark.DataFrame): immigration fact table.\n",
    "    \"\"\"\n",
    "    \n",
    "    #### step1: extract a map of (field --> value) from the I94_SAS_Labels_Descriptions.SAS file\n",
    "    ####        and decode columns of the immigration dataset.\n",
    "    \n",
    "    all_fields_maps = map_features_extraction('I94_SAS_Labels_Descriptions.SAS')\n",
    "    immigration = immigration.withColumn('i94cit', col('i94cit').cast('string'))\\\n",
    "                             .replace(all_fields_maps['i94cit'], subset=['i94cit'])\n",
    "    immigration = immigration.withColumn('i94res', col('i94res').cast('string'))\\\n",
    "                             .replace(all_fields_maps['i94cit'], subset=['i94res'])\n",
    "    immigration = immigration.withColumn('i94mode', col('i94mode').cast('string'))\\\n",
    "                             .replace(all_fields_maps['i94mode'], subset=['i94mode'])\n",
    "    immigration = immigration.withColumn('I94VISA', col('I94VISA').cast('string'))\\\n",
    "                             .replace(all_fields_maps['I94VISA'], subset=['I94VISA'])\n",
    "    immigration = immigration.replace(all_fields_maps['i94port'], subset=['i94port'])\n",
    "    \n",
    "    #### step2: extracts the city name from the 'i94port' column after decoding in step1.\n",
    "    immigration = immigration.withColumn('city', extract_city(col('i94port')))\n",
    "\n",
    "    #### step3: drop nans from arrival and departure dates (from immigration dataset) and convert them from SAS to date format.\n",
    "    date_fields = ['arrdate', 'depdate']\n",
    "    immigration = immigration.dropna(subset=(date_fields))\n",
    "    immigration = immigration.withColumn('arrdate', convertSaS_toDate(col('arrdate')))\n",
    "    immigration = immigration.withColumn('depdate', convertSaS_toDate(col('depdate')))\n",
    "\n",
    "    #### step4: select a set of columns to be included in the fact table.\n",
    "    selected_fields = [\n",
    "        'cicid', 'arrdate', 'depdate', 'i94cit', \n",
    "        'i94res', 'biryear', 'gender', 'i94mode',\n",
    "        'airline', 'fltno', 'visatype', \n",
    "        'i94visa', 'i94addr', 'city'\n",
    "    ]\n",
    "    fact_immigration = immigration.select(selected_fields)\n",
    "    \n",
    "    #### step5: set all city values to lower case and all state_code values to upper case.\n",
    "    fact_immigration = fact_immigration.withColumn('city', lower(col('city')))\n",
    "    fact_immigration = fact_immigration.withColumn('i94addr', upper(col('i94addr')))\\\n",
    "                                       .withColumnRenamed('i94addr', 'state_code')\n",
    "    \n",
    "    #### step6: create a view of the immigration fact table and join it with the time, cities_dimographics and cities_airports\n",
    "    ####        dimensions tables to add the foreign keys. \n",
    "    fact_immigration.createOrReplaceTempView('fact_immigration')\n",
    "    \n",
    "    fact_immigration = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT aprt.dim_airport_pk dim_airport_fk, \n",
    "        dem.dim_cities_demographics_pk dim_cities_demographics_fk, \n",
    "        time.arrival_date_pk arrival_date_fk, cicid cicid_pk, depdate departure_date, \n",
    "        i94cit, i94res, biryear, gender, airline, fltno, \n",
    "        visatype, i94visa, i94mode\n",
    "        FROM fact_immigration as im\n",
    "        JOIN dim_cities_demographics as dem\n",
    "        ON (im.state_code = dem.state_code) and (im.city = dem.city)\n",
    "        JOIN dim_cities_airport as aprt\n",
    "        ON (im.state_code = aprt.state_code) and (im.city = aprt.city)\n",
    "        JOIN dim_time as time\n",
    "        ON (im.arrdate = time.arrival_date_pk)\n",
    "    \"\"\")\n",
    "    \n",
    "    return fact_immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Run Data Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create views\n",
    "airport.createOrReplaceTempView('airport')\n",
    "immigration.createOrReplaceTempView('immigration')\n",
    "demographics.createOrReplaceTempView('demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dim_airport_pk: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- total_large_airport: long (nullable = true)\n",
      " |-- total_medium_airport: long (nullable = true)\n",
      " |-- total_small_airport: long (nullable = true)\n",
      " |-- total_balloonport: long (nullable = true)\n",
      " |-- total_seaplane_base: long (nullable = true)\n",
      " |-- total_heliport: long (nullable = true)\n",
      " |-- total_closed: long (nullable = true)\n",
      " |-- total_airports: long (nullable = true)\n",
      " |-- avg_elevation_ft: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_airport_pk</th>\n",
       "      <th>state_code</th>\n",
       "      <th>city</th>\n",
       "      <th>total_large_airport</th>\n",
       "      <th>total_medium_airport</th>\n",
       "      <th>total_small_airport</th>\n",
       "      <th>total_balloonport</th>\n",
       "      <th>total_seaplane_base</th>\n",
       "      <th>total_heliport</th>\n",
       "      <th>total_closed</th>\n",
       "      <th>total_airports</th>\n",
       "      <th>avg_elevation_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK, adak island</td>\n",
       "      <td>AK</td>\n",
       "      <td>adak island</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK, akhiok</td>\n",
       "      <td>AK</td>\n",
       "      <td>akhiok</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK, akiachak</td>\n",
       "      <td>AK</td>\n",
       "      <td>akiachak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AK, akiak</td>\n",
       "      <td>AK</td>\n",
       "      <td>akiak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AK, akutan</td>\n",
       "      <td>AK</td>\n",
       "      <td>akutan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dim_airport_pk state_code         city  total_large_airport  \\\n",
       "0  AK, adak island         AK  adak island                    0   \n",
       "1       AK, akhiok         AK       akhiok                    0   \n",
       "2     AK, akiachak         AK     akiachak                    0   \n",
       "3        AK, akiak         AK        akiak                    0   \n",
       "4       AK, akutan         AK       akutan                    0   \n",
       "\n",
       "   total_medium_airport  total_small_airport  total_balloonport  \\\n",
       "0                     1                    0                  0   \n",
       "1                     0                    1                  0   \n",
       "2                     0                    1                  0   \n",
       "3                     0                    1                  0   \n",
       "4                     0                    1                  0   \n",
       "\n",
       "   total_seaplane_base  total_heliport  total_closed  total_airports  \\\n",
       "0                    0               0             0               1   \n",
       "1                    0               0             0               1   \n",
       "2                    1               0             0               2   \n",
       "3                    0               0             0               1   \n",
       "4                    1               0             0               2   \n",
       "\n",
       "   avg_elevation_ft  \n",
       "0              18.0  \n",
       "1              44.0  \n",
       "2              11.5  \n",
       "3              30.0  \n",
       "4               NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_cities_airports = process_dim_cities_airports(spark, airport)\n",
    "dim_cities_airports.printSchema()\n",
    "dim_cities_airports.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: double (nullable = true)\n",
      " |-- female_population: double (nullable = true)\n",
      " |-- total_population: long (nullable = true)\n",
      " |-- number_of_veterans: double (nullable = true)\n",
      " |-- foreign_born: double (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- american_indian_and_alaska_native_population: long (nullable = true)\n",
      " |-- asian_population: long (nullable = true)\n",
      " |-- black_or_african_american_population: long (nullable = true)\n",
      " |-- hispanic_or_latino_population: long (nullable = true)\n",
      " |-- white_population: long (nullable = true)\n",
      " |-- dim_cities_demographics_pk: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>american_indian_and_alaska_native_population</th>\n",
       "      <th>asian_population</th>\n",
       "      <th>black_or_african_american_population</th>\n",
       "      <th>hispanic_or_latino_population</th>\n",
       "      <th>white_population</th>\n",
       "      <th>dim_cities_demographics_pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bayonne</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>39.7</td>\n",
       "      <td>32705.0</td>\n",
       "      <td>33598.0</td>\n",
       "      <td>66303</td>\n",
       "      <td>2225.0</td>\n",
       "      <td>21899.0</td>\n",
       "      <td>2.62</td>\n",
       "      <td>NJ</td>\n",
       "      <td>0</td>\n",
       "      <td>7171</td>\n",
       "      <td>7581</td>\n",
       "      <td>19525</td>\n",
       "      <td>41431</td>\n",
       "      <td>NJ, bayonne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>canton</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>33.4</td>\n",
       "      <td>34476.0</td>\n",
       "      <td>37419.0</td>\n",
       "      <td>71895</td>\n",
       "      <td>3404.0</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>2.32</td>\n",
       "      <td>OH</td>\n",
       "      <td>358</td>\n",
       "      <td>0</td>\n",
       "      <td>21353</td>\n",
       "      <td>3597</td>\n",
       "      <td>53571</td>\n",
       "      <td>OH, canton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carlsbad</td>\n",
       "      <td>California</td>\n",
       "      <td>42.1</td>\n",
       "      <td>55119.0</td>\n",
       "      <td>58347.0</td>\n",
       "      <td>113466</td>\n",
       "      <td>6031.0</td>\n",
       "      <td>17689.0</td>\n",
       "      <td>2.68</td>\n",
       "      <td>CA</td>\n",
       "      <td>1513</td>\n",
       "      <td>11948</td>\n",
       "      <td>876</td>\n",
       "      <td>12969</td>\n",
       "      <td>98705</td>\n",
       "      <td>CA, carlsbad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>colorado springs</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>34.8</td>\n",
       "      <td>225544.0</td>\n",
       "      <td>231018.0</td>\n",
       "      <td>456562</td>\n",
       "      <td>49291.0</td>\n",
       "      <td>35320.0</td>\n",
       "      <td>2.48</td>\n",
       "      <td>CO</td>\n",
       "      <td>11146</td>\n",
       "      <td>22619</td>\n",
       "      <td>38976</td>\n",
       "      <td>78711</td>\n",
       "      <td>374900</td>\n",
       "      <td>CO, colorado springs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>des moines</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>34.5</td>\n",
       "      <td>103726.0</td>\n",
       "      <td>106591.0</td>\n",
       "      <td>210317</td>\n",
       "      <td>11780.0</td>\n",
       "      <td>23857.0</td>\n",
       "      <td>2.41</td>\n",
       "      <td>IA</td>\n",
       "      <td>3964</td>\n",
       "      <td>15205</td>\n",
       "      <td>26353</td>\n",
       "      <td>25306</td>\n",
       "      <td>168057</td>\n",
       "      <td>IA, des moines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city       state  median_age  male_population  \\\n",
       "0           bayonne  New Jersey        39.7          32705.0   \n",
       "1            canton        Ohio        33.4          34476.0   \n",
       "2          carlsbad  California        42.1          55119.0   \n",
       "3  colorado springs    Colorado        34.8         225544.0   \n",
       "4        des moines        Iowa        34.5         103726.0   \n",
       "\n",
       "   female_population  total_population  number_of_veterans  foreign_born  \\\n",
       "0            33598.0             66303              2225.0       21899.0   \n",
       "1            37419.0             71895              3404.0        2204.0   \n",
       "2            58347.0            113466              6031.0       17689.0   \n",
       "3           231018.0            456562             49291.0       35320.0   \n",
       "4           106591.0            210317             11780.0       23857.0   \n",
       "\n",
       "   average_household_size state_code  \\\n",
       "0                    2.62         NJ   \n",
       "1                    2.32         OH   \n",
       "2                    2.68         CA   \n",
       "3                    2.48         CO   \n",
       "4                    2.41         IA   \n",
       "\n",
       "   american_indian_and_alaska_native_population  asian_population  \\\n",
       "0                                             0              7171   \n",
       "1                                           358                 0   \n",
       "2                                          1513             11948   \n",
       "3                                         11146             22619   \n",
       "4                                          3964             15205   \n",
       "\n",
       "   black_or_african_american_population  hispanic_or_latino_population  \\\n",
       "0                                  7581                          19525   \n",
       "1                                 21353                           3597   \n",
       "2                                   876                          12969   \n",
       "3                                 38976                          78711   \n",
       "4                                 26353                          25306   \n",
       "\n",
       "   white_population dim_cities_demographics_pk  \n",
       "0             41431                NJ, bayonne  \n",
       "1             53571                 OH, canton  \n",
       "2             98705               CA, carlsbad  \n",
       "3            374900       CO, colorado springs  \n",
       "4            168057             IA, des moines  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_cities_demographics = process_dim_cities_demographics(spark, demographics)\n",
    "dim_cities_demographics.printSchema()\n",
    "dim_cities_demographics.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival_date_pk: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- is_weekend: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrival_date_pk</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-07-20</td>\n",
       "      <td>2001</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-12</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-14</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-04-22</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  arrival_date_pk  year  month  day  weekofyear  weekday  season  is_weekend\n",
       "0      2001-07-20  2001      7   20          29        6       3           0\n",
       "1      2012-04-12  2012      4   12          15        5       2           0\n",
       "2      2012-04-14  2012      4   14          15        7       2           1\n",
       "3      2014-04-22  2014      4   22          17        3       2           0\n",
       "4      2014-04-24  2014      4   24          17        5       2           0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_time = process_dim_time(spark, immigration)\n",
    "dim_time.printSchema()\n",
    "dim_time.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough values to unpack (expected 2, got 1)  value: ;\n",
      "\n",
      "root\n",
      " |-- dim_airport_fk: string (nullable = true)\n",
      " |-- dim_cities_demographics_fk: string (nullable = true)\n",
      " |-- arrival_date_fk: date (nullable = true)\n",
      " |-- cicid_pk: double (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      "\n",
      "+--------------+--------------------------+---------------+---------+--------------+--------------+--------------+-------+------+-------+-----+--------+--------+-------+\n",
      "|dim_airport_fk|dim_cities_demographics_fk|arrival_date_fk| cicid_pk|departure_date|        i94cit|        i94res|biryear|gender|airline|fltno|visatype| i94visa|i94mode|\n",
      "+--------------+--------------------------+---------------+---------+--------------+--------------+--------------+-------+------+-------+-----+--------+--------+-------+\n",
      "|    MA, boston|                MA, boston|     2016-04-25|4708984.0|    2016-05-07|         148.0|       GERMANY| 2010.0|  null|     LH|00422|      WT|Pleasure|    Air|\n",
      "|    MA, boston|                MA, boston|     2016-04-25|4730605.0|    2016-06-13|    CHINA, PRC|    CHINA, PRC| 1999.0|     M|     HU|00481|      F1| Student|    Air|\n",
      "|    MA, boston|                MA, boston|     2016-04-25|4696402.0|    2016-05-05|UNITED KINGDOM|UNITED KINGDOM| 1948.0|     F|     DY|07147|      WT|Pleasure|    Air|\n",
      "|    MA, boston|                MA, boston|     2016-04-25|4716673.0|    2016-04-28|         JAPAN|         JAPAN| 1966.0|     M|     JL|00008|      WB|Business|    Air|\n",
      "|    MA, boston|                MA, boston|     2016-04-25|4732583.0|    2016-06-10|    CHINA, PRC|    CHINA, PRC| 1997.0|     M|     HU|  481|      F1| Student|    Air|\n",
      "+--------------+--------------------------+---------------+---------+--------------+--------------+--------------+-------+------+-------+-----+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration = process_fact_immigration(spark, immigration)\n",
    "fact_immigration.printSchema()\n",
    "fact_immigration.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    " \n",
    "Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Unique primary keys check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def unique_key_check(spark_data, spark_data_label, key):\n",
    "    \"\"\"\n",
    "    Checks if a key is unique or not by printing a message.\n",
    "    ---\n",
    "    \n",
    "    args:\n",
    "        spark_data(Spark.DataFrame): spark dataframe representing a fact or dimension table.\n",
    "        spark_data_label(string): the dataframe label.\n",
    "        key(string): the unique key field.\n",
    "    \n",
    "    returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    \n",
    "    if spark_data.count() - spark_data.select(key).distinct().count() == 0:\n",
    "        print(f'{spark_data_label}: unique primary key test passed')\n",
    "    else:\n",
    "        print(f'{spark_data_label}: unique primary key test failed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "unique_key_check(fact_immigration, 'fact_immigration', 'cicid_pk')\n",
    "unique_key_check(dim_time, 'dim_time', 'arrdate_pk')\n",
    "unique_key_check(dim_cities_demographics, 'dim_cities_demographics', 'dim_cities_demographics_pk')\n",
    "unique_key_check(dim_cities_airports, 'dim_cities_airports', 'dim_airport_pk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " - Check if there are any nans in primary and foreign keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_missing_values(spark_data, spark_data_label, subset):\n",
    "    \"\"\"\n",
    "    Counts the missing values of a spark data frame primary/foreign keys\n",
    "    and prints a message to indicate if the test passed or not.\n",
    "    ---\n",
    "    \n",
    "    args:\n",
    "        spark_data(Spark.DataFrame): spark dataframe representing a fact or dimension table.\n",
    "        spark_data_label(string): the dataframe label.\n",
    "        subset(list): a list of one or multiple column names.\n",
    "    \n",
    "    returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    print(f\"\"\"\n",
    "        {spark_data_label} keys missing values test passed.\n",
    "    \"\"\"\n",
    "    if spark_data.select([\n",
    "        (count(when(col(c).isNull(), c))/count(lit(1))).alias(c) \n",
    "        for c in subset\n",
    "    ]).toPandas().sum().sum() == 0 else f\"\"\"\n",
    "        {spark_data_label}: keys missing values test failed!\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "check_missing_values(fact_immigration, 'fact_immigration', ['cicid_pk', 'arrdate_fk', 'dim_cities_demographics_fk', 'dim_airport_fk'])\n",
    "check_missing_values(dim_time, 'dim_time', ['arrdate_pk'])\n",
    "check_missing_values(dim_cities_demographics, 'dim_cities_demographics', ['dim_cities_demographics_pk'])\n",
    "check_missing_values(dim_cities_airports, 'dim_cities_airports', ['dim_airport_pk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Check if tables are not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f' dim_time rows: {dim_time.count()}')\n",
    "print(f'dim_cities_airports rows: {dim_airports.count()}')\n",
    "print(f'dim_cities_demographics: {dim_cities_demographics.count()}')\n",
    "print(f'fact_immigration rows: {fact_immigration.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Write Data Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# AWS configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('aws_config.cfg', encoding='utf-8-sig')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "output_path = config['S3']['S3_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write airports to S3 and partition data by sate_code and city.\n",
    "dim_cities_airports.write.partitionBy('state_code', 'city')\\\n",
    "                     .mode('overwrite')\\\n",
    "                     .parquet(os.path.join(output_path, 'dim_cities_airports'))\n",
    "\n",
    "#write dim_cities_demographics to S3 and partition data by sate_code and city.\n",
    "dim_cities_demographics.write.partitionBy('state_code', 'city')\\\n",
    "                     .mode('overwrite')\\\n",
    "                     .parquet(os.path.join(output_path, 'dim_cities_demographics'))\n",
    "\n",
    "#write dim_time to S3.\n",
    "dim_time.write.mode('overwrite')\\\n",
    "              .parquet(os.path.join(output_path, 'dim_time'))\n",
    "\n",
    "#write fact_immigration to S3 and partition data by airport foreign key (dim_airport_fk).\n",
    "fact_immigration.write.partitionBy('dim_airport_fk')\\\n",
    "                      .mode('overwrite')\\\n",
    "                      .parquet(os.path.join(output_path, 'fact_immigration'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "<b> dim_cities_demographics: </b><br>\n",
    "* <b>dim_cities_demographics_pk:</b> Dimension table pripary key composed of state_code + city label.\n",
    "* <b>city:</b> City label.\n",
    "* <b>state:</b> State label.\n",
    "* <b>state_code:</b> 2 character unique state code.\n",
    "* <b>median_age:</b> City population median age.\n",
    "* <b>male_population:</b> Total number of the city male population.\n",
    "* <b>female_population:</b> Total number of the city female population.\n",
    "* <b>total_population:</b> Total number of city population.\n",
    "* <b>number_of_veterans:</b> Total number of city veterans.\n",
    "* <b>foreign_born:</b> Total number of city foreign borns.\n",
    "* <b>average_household_size:</b> Average of persons per household per city.\n",
    "* <b>hispanic_or_latino_population:</b> Total number of hispanic or latino race population per city.\n",
    "* <b>white_population:</b> Total number of white race population per city.\n",
    "* <b>asian_population:</b> Total number of asian race population per city.\n",
    "* <b>black_or_african_american_population:</b> Total number of black or african american race population per city.\n",
    "* <b>american_indian_and_alaska_native_population:</b> Total number of american indian and alaska native race population per city.\n",
    "\n",
    "<b> dim_cities_airports: </b><br>\n",
    "* <b>dim_airport_pk:</b> Dimension table pripary key composed of state_code + city label.\n",
    "* <b>city:</b> City label.\n",
    "* <b>state_code:</b> 2 character unique state code.\n",
    "* <b>total_large_airport:</b> Total number of large airports per city.\n",
    "* <b>total_medium_airport:</b> Total number of medium airports per city.\n",
    "* <b>total_small_airport:</b> Total number of small airports per city.\n",
    "* <b>total_balloonport:</b> Total number of balloon port pere city.\n",
    "* <b>total_seaplane_base:</b> Total number of seaplane bases per city.\n",
    "* <b>total_heliport:</b> Total number of heliport per city.\n",
    "* <b>total_airports:</b> Total number of active airports per city.\n",
    "* <b>total_closed:</b> Total number of closed airports per city.\n",
    "* <b>avg_elevation_ft:</b> Average of airports/bases elevation feat per city.\n",
    "\n",
    "<b> dim_time: </b><br>\n",
    "* <b>arrival_date_pk:</b> Dimension table pripary key representing arrival and departure dates.\n",
    "* <b>year:</b> Date year, extracted from the arrival/departure date.\n",
    "* <b>month:</b> Month of the year, extracted from the arrival/departure date.\n",
    "* <b>day:</b> Day of month, extracted from the arrival/departure date.\n",
    "* <b>weekday:</b> Day of week, extracted from the arrival/departure date.\n",
    "* <b>weeofyear:</b> indicates the week year number.\n",
    "* <b>season:</b> The season of the year, extracted from the arrival/departure date.\n",
    "* <b>isweekend:</b> indicates whether a day is a week-end or not.\n",
    "\n",
    "<b> fact_immigration: </b><br>\n",
    "* <b>cicd_pk:</b> immigration fact primary key, indicates whether a day is a week-end or not.\n",
    "* <b>arrival_date_fk:</b> immigration fact foreign key, links the fact table with the time dimension.\n",
    "* <b>dim_airport_fk:</b>  immigration fact foreign key, links the fact table with the cities_airports dimension.\n",
    "* <b>dim_cities_demographics_fk:</b>  immigration fact foreign key, links the fact table with the cities_demographics dimension.\n",
    "* <b>departure_date:</b> the departure date from the US.\n",
    "* <b>i94res:</b> immigrant residence country.\n",
    "* <b>i94cit:</b> immigrant birth country.\n",
    "* <b>biryear:</b> 4 digit year of birth.\n",
    "* <b>gender:</b> non-immigrant sex.\n",
    "* <b>airline:</b> airline used to arrive in U.S.\n",
    "* <b>fltno:</b> the flight number.\n",
    "* <b>i94visa:</b> visa codes indicating the reason for the immigration/travel.\n",
    "* <b>i94mode:</b> indicates the means of transport used to move to/from the U.S.\n",
    "* <b>visatype:</b> class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "- Choice of tools and technologies for the project:\n",
    "    - During this project I used Pandas and Pyspark to process our data. I used both tools mainly due to the volume of the data. For example, the immigration dataset couldn't be processed using Pandas and Pyspark was a good way to go. Also, I used S3 to store the data model. It was a good choice as the aws service had a low cost and is designed for storing large volumes of data.\n",
    "- How often the data should be updated and why?\n",
    "    - The dim_cities_airports data should be updated at least once in 2 years, that's because the airports numbers will barely change in each city if they ever change.\n",
    "    - The dim_cities_demographics data should be updated at least once a year, that's because we can notice a slight change in the stats from one year to another with regard to this data.\n",
    "    - The fact_immigration data should be updated every month, that's because we can notice a high volume of data registered every month.\n",
    "- How would I approach the problem differently under the following scenarios ?\n",
    "    - The data was increased by 100x:\n",
    "        - I managed to process this data using spark in a standalone mode, but if the data increased by 100x or more, I will have to switch to the cluster mode using EMR aws service for example.\n",
    "    - The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "        - In such a case, I will need a framework that schedules data pipeline executions on a daily basis. I will use Apache Airflow to create a DAG capable of managing these pipelines.\n",
    "    - The database needed to be accessed by 100+ people.\n",
    "        - If our database needs to be accessed by lots of people and in a short period of time, then I will store the data on Redshift or BigQuery which are both designed to manage a data warehouse and are optimized for access and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
